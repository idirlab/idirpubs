We find many critical comments invalid. We eagerly and humbly request reviewers to verify and discuss. 

Review 1
--------
-"This paper uses the notion of indifference, meaning that two objects are equally preferred. It is different from incomparability, meaning that two objects do not dominate each other."  "... confused notions between incomparability and indifference: in page 1, the notion of indifference is that two objects are equally preferred. Meanwhile, in page 5 (intransitivity of object dominance), the meaning of indifference is similar to incomparability."

We use the notion of "indifference" in the same way as in the literature, e.g., [2]. It is consistently used throughout the paper. 

In our paper, "indifference" means two objects are not better than each other, with regard to a criterion c. As formally stated in the 1st paragraph of Sec. I, x and y are "indifferent" regarding c, if neither (x,y) nor (y,x) belongs to the preference relation P_c. When x and y are indifferent, the reason (and thus the interpretation) could be that "x and y are equally good or incomparable with regard to c", as stated in the 1st paragraph too. However, in modeling preference by a better-than relation (which is what the paper adopts), it doesn't distinguish "equally good" and "incomparable". 

A more expressive exposure is to model preference by a not-worse-than relation (perhaps what you meant by "bi-directed arrows"), which can distuiuish "equally good" and "incomparable"

In the literature, some studies use better-than relation (e.g., [2]) to model partial-order preferences, others use not-worse-than relation. We would be happy to include such a discussion and clarification in the paper. We also conjecture that our framework and algorithms are applicable on such relations too. 

Review 3
--------
-"The problem of aggregating partial to find the total order is not new." "crowdsourcing-based skyline queries have been studied" "solutions are not novel"

The paper's novelty is discussed in detail in the paragraph and bullets after Example 3, in Sec. II (Related work), and in Table I. Particularly, (1) we don't aggregate partial orders to find total order; instead, we aggregate pairwise comparison results to derive partial orders on individual criteria and further find pareto-optimal objects based on the multiple partial orders; (2) the only crowdsourcing-based skyline query paper is [8], as its difference from our work is clearly given in Table I and the aforementioned places; (3) if the reviewer points out existing solutions similar to ours, we would be happy to investigate. 

-"1. The authors did not prove the hardness of the proposed problem, I mean selecting the minimum number of questions for finding pareto optimal objects."

The proposed problem is "finding pareto optimal objects". Number-of-questions is the performance measure and we aim to use as few questions as possible. As explained in the 3rd paragraph of Sec. III, the worst number of questions required is r*n*(n-1)/2. So the problem is polynomial-time. It is not an NP-hard problem. 

Note that the proposed problem is not "selecting the minimum number of questions". The input to the problem contains no information about the preference relations. (Hence crowdsourcing is used to figure out such information.) If we set the problem as "finding the minimum number of questions", it would be like finding the shortest path between 2 nodes in a graph without seeing the edges. You can figure out the shortest path, say, by breadth-first traversal. But you won't be able to avoid exploring edges not on the shortest path (thus won't be able to guarantee minimum questions), without knowing anything about the edges to begin with.

-"2. The proposed heuristic solutions do not have bound to the optimal solutions."

As pointed out in the 3rd paragraph of Sec. III, in the worst case, any solution will have to ask all possible r*n*(n-1)/2 questions. 

It is also worth mentioning that in Theorem 2 we proved a lower bound on the number of required questions. 

-"3. The experimental study is very limited." "the authors only used simulation to study the effectiveness of different parameters."

We actually did experiments using real crowdsourcing platforms and human judges (Sec. V-B "case studies"). We'd like to point out large-scale experiments on crowdsourcing platforms is not affordable to most academics. It is not uncommon to find crowdsourcing papers in ICDE-like venues that collect hundreds/thousands (instead of millions) of inputs from real crowdsourcers or even only use simulations.  

-"4. The authors assumed that the workers are 100% correct, which is often not the case."

We didn't make the assumption. We derive preference relations from multiple workers (Equation (1), page 5), exactly because they are not 100% correct. That's also why we did quality control in Case study 2 (the last paragraph of Sec. V): "...A crowdsourcerâ€™s input is discarded if..."


Review 2
--------
-"D2. In Section IV, Q^1_{can} appears suddenly."

Q^1_{can} first appeared in Algorithm 2. We will mention this in the text too. 

-"D3. In Section V-A(2), the ratio of pairwise comparisons required by RandomQ to that used by BruteForce is 0.0048, which is the same as that used by FRQ. This is strange."

Actually, the 2 values are different: 0.0048 and 0.00048.
