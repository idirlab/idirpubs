Dear Mohamed and PC Chairs,

Thank you for getting back to us. That means a lot already. 

Low-quality reviews do happen all the time. However, this is among the worst I encountered. I had chosen to live with it in previous incidents, but it is just unbearable this time since most of the critical comments and ratings are invalid and the flaws are apparent! There was no real discussion between the reviewers. The message included in your email (from Reviewer 3, judging by the content) resorts to sophistry and nitpicking, and it does not touch upon many baseless statements in the review. Our feedback also pointed out the critical problems in Review 1, but there was no discussion from the reviewer. Both Reviews 1 and 3 remain identical. The reviews' problems are not due to the paper's writing, which I hope you will agree with after comparing the writings of some reviews and the paper.

The evidence of poor reviewing is strong and the discrepancy between the reviews is large. In a case as such, if the reviewers do not discuss and do not realize or acknowledge mistakes, should the conference just let the scores/outcome stay? I believe that sends wrong messages to everyone. It tells reviewers sloppy reviews are okay; it tells authors to accept it as norms of our community. It is particularly discouraging to students when they are dismissed in such a way after one year's hard-work and an anxious wait of several months and they have to consider it normal. Personally speaking, poor reviewing has hurt my career and at times has lowered my enthusiasm for the field and the profession. 

I sincerely hope you could do something about it. That's why I spent many hours (8 hours for feedback, 6 hours for my last email, and 8 hours today) to read the paper again, to analyze the reviews and try really hard to best-guess what some vague comments might mean, and to make my feedback and emails as accurate, clear and objective as I am able to. I also realize the status quo has been like this for a while and it really requires everyone to do something. Hence, I am doing it, by diligently following up on the matter.

If you are willing to look deeper into this (which I humbly request), it will be highly appreciated. Details were described in our earlier letter to you. Below we focus on Reviewer 3's message in your email. 

(1) First of all, Review 3 gives the paper really harsh ratings without any explanation ("Sub-standard: would require heavy rewrite" on presentation, "Same ideas published before (say where)" on Novelty, "No Impact" on Significance). (We also note that Review 2 gives the highest ratings on most categories.)  The review also says "crowdsourcing-based skyline queries have been studied" and "The problem of aggregating partial to find the total order is not new." But those are not the problems studied in the paper. The paper explains in detail how these problems are related to and different from ours. 

(2) Review 3 contained a comment on the "hardness of the proposed problem". Our feedback pointed out it is invalid. Reviewer 3's message doesn't talk about it, but the comment remains in the review. 

(3) The review said that we assumed 100% correct crowd workers. Our feedback points out we didn't make the assumption and we did quality control. Now the message is instead criticizing us for not having 100% correct crowd workers! It says "even we ask the crowd workers to pass some qualification test before accepting the real jobs, the accuracy is still not 100%. Unless you use the same questions as your qualification test questions, which is apparently not the case in this paper."  This is wrong in two ways: (3.a) They incorrectly assumed we did quality control before asking real questions. In fact, we did exactly what they mentioned as using "the same questions as qualification test questions". Our paper (last paragraph in Section V) clearly says such questions were mixed together with other questions. The paper further explains they are just the same type of questions as the regular ones. Unlike regular questions, these questions have expected answers and thus workers who answer differently can be detected as low-quality workers. (3.b) The reviewer's message seems to suggest that we would have 100% correct workers if we screen workers in the way preferred by the reviewer (which is exactly what we did, as explained in (3.a)). How could it guarantee 100% correct workers? 

(4) With regard to Case study 1, the message says the first case study "is not repeatable". But on what basis does the reviewer say that and why is using Amazon Mechanical Turk (AMT) repeatable? If it is because others won't have access to the same local participants used in case study 1, isn't that they won't be able to use the same AMT workers used in case study 2 either?

With regard to Case study 2, the message insists that we must use AMT to study the effect of different parameters and the cost would be very cheap. That's not true. Given n objects and r criteria, there are r*n*(n-1)/2 possible questions. Each question is answered by k crowdsourcers. If each question requires x cents, the cost for a study on a particular combination of (n, r, k) values is x*k*r*n*(n-1)/2. In our study we group questions into forms, each of which containing 9 questions, and we paid 50 cents for each form. Hence we paid 50/9 cents for each question. Suppose we want to study the effect of various combinations of (n,r,k), with largest values being (n=100,r=10,k=10) which are actually quite small. The cost for just that one combination would be $27500! Even if we pay only 1 cent per question, it would still cost $4950. Note that this is only for 1 combination of parameter values, and it doesn't really test on large parameter values. 

The purpose of our case studies is to show how the ideas can be used in real scenarios and how the Hasse diagrams make real sense. The experiments on synthetic data (NBA) study scalability. We did do the above calculation and decided we couldn't reasonably afford it. As we explained in the feedback, it is not affordable to most academics. Many crowdsourcing papers in ICDE-alike venues even only use simulations.

(5) With regard to "bound": 

(5.a) The reviewer's message says Theorem 2 "stated the number of necessary pairwise comparison questions. It is not about their algorithms performance."  But, the paper clearly mentions that number-of-questions is our performance measure and "the focus shall be on how to find a short question sequence instead of the algorithmsâ€™ complexity." After all, a brute-force algorithm would just enumerate all possible questions.  The time for enumerating questions is effectively negligible, in comparison to the time needed for answering questions by humans. 

(5.b) The message also says "I am asking how far the proposed solution to the optimal one" and "they never show how close the proposed solution to that of the brute-force approach."  

It is less clear what exactly the reviewer meant by "optimal one" and "optimal solution" in the review/message. The reviewer indeed has mistakenly considered our problem as "selecting the minimum number of questions" (pointed out in our feedback) and considered algorithm complexity as our performance measure (pointed out in (5.a)). The reviewer might have two different notions of "optimal solution". 

-- If the reviewer is referring to a method that always finds the smallest number of needed questions:  Our feedback explained that no method can possibly guarantee not asking unnecessary questions, due to the unknown preference relations to begin with. Such optimal solution doesn't exist.

-- If the reviewer is referring to the smallest number of questions needed (let's call it the "minimum"): there is no known equations bounding the gaps between the minimum, the number-of-questions required by the proposed methods, and that required by the brute-force method. In fact, such gaps are data-dependent. There are preference relations on which every method must use all r*n*(n-1)/2 possible questions (as explained in page 4 of the paper). In such cases, there is no gap between the minimum, the brute-force and any method. 

Hence, we believe making such bounds must-have for the paper is very harsh. Nevertheless, Theorem 2 proves a lower-bound on the number of questions needed by any method based on pairwise comparisons. The experiments results further demonstrate the small gap between the proposed methods and the minimum (which must be between the proposed methods and the lower-bound). As the paper points out, the results "reveal that FRQ is nearly optimal and the lower bound is practically tight, since FRQ gets very close to the lower bound." The results also shows orders-of-magnitude gap between the proposed methods and the brute-force. 


Thank you very much for your time!

Best regards,
Chengkai