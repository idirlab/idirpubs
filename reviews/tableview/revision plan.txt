Plan:

(1) User study:

* real human beings (graduate students, ideally familiar with knowledge graphs)
* compare preview tables (concise vs. concise+tight vs. concise+diverse) vs. schema graph vs. summary of schema graph (VLDB09?) vs. freebase vs. hand-crafted
* do we need to include ObjectRank in comparison?
* design: 
1. show the results from each method. 
- previw tables: something similar to the web based demo; concise vs. concise+tight vs. concise+diverse
- schema graph: Ning alreay had that?  show example entities beside each node?
- summary of schema graph: how to present clustering results? show example entities?
- freebase groundtruth: do we still have that? 
- hand-crafted: show data graph to users, and ask users to manaully produce preview tables using the same paramters.
2. What question to ask?
3. What measures to use?  Quality, time, ...
4. post-hoc analysis: let user read the data graph a bit, and judge which method provided better overview?
* estimate: How many users do we need to use? How many tasks for each? how much time/money?

(2) bigger schema graphs: which ones can we use?  merge multiple domains? did Ning already try that?

(3) provide more detailed information on Freebase ground truth, schema summary graph, AMT workers. Do we alreay have all the information? Clarify with Ning.

(4) Related work:  check the following 3 papers and understand what you need to discuss (hopefully not implementation and experiment)

* "Qunits: queried units in database search", Nandi and Jagadish CIDR 2009
* "Query Biased Snippet Generation in XML Search", Chen et al SIGMOD 2009
* ObjectRank, Balmin @ VLDB 2004

(5) measure execution time of VLDB09

1.W3: more comparisons against reasonable baselines / comparable algorithms; the evaluations (Fig 6 & 7) simply compare the two proposed algorithms, but do not compare this against a reasonable existing alternative.

(6) tuple sampling approach:
2.W1: Selecting sample tuples is very likely an important part of the puzzle and it was disappointing to see the authors use random sampling.

---------------------------------------------------------------------------------------------------------------

Reviews:

(1) User study:
=========================
3.R1: In the experiments, compare the usefulness of the preview tables vs schema graph (or summary of the schema graph). This can be done by a user study among data workers (and not AMT users).

2.R1: The user evaluation has be more convincing. Specially, it would make sense to quantitatively compare the preview tables computed by the algorithms against hand-crafted schemas created by a set of experts.

2.D6: An alternate evaluation strategy might be a have a set of 10 experts (e.g., grad students with some data management understanding) independently create preview tables for the said datasets under the outlined design constraints (k tables, n attributes). These can be instead compared with the results of the algorithm (and correlation/overlap measured).

2.D2: It would be more convincing if the paper included a evaluation in which the Freebase schema was also rated in the user study and then the overlap measure with the solutions generated by the different algorithms.

2.R2: The differences between the variations in criterion (concise + diverse + dense) would ideally also be evaluated.

1.W2: Motivation: demonstrate that intuitive alternatives do not suffice. 
-- Pagerank representation of the graph (ObjectRank, Balmin @ VLDB 2004) 
-- schema summarization etc
-- show that this method is *faster*(i.e. time-to-task* than human curation?

1.R1: The focus of this paper could be improved significantly if the authors focused on the qualitative aspects and the human curation aspects instead of the performance experiments. 

1.R2: The current user study is not enough to tell how this will indeed help users to make a decision to select the entity graph. It is only used to evaluate the ranking. The paper needs to show that the technique provided is a good qualitative alternative to the baseline / naive case.

(2) Use bigger scheam graphs: 
=================================
3.W2: The schema graphs used in the experiments are not big. Considering the fact that preview tables are useful when users are dealing with big schema graphs, this is not acceptable.

(3) More detailed information related to existing experiments: 
=================================
2.D1. It is hard to appreciate the quality comparison, when the Freebase schemas used are unknown (as the author suggests, the published pages are no longer accessible). At the very least the authors should list the entity types and the non-key attributes for a few of the tested domains.

3.R2: In the experiments, a sample of the schema graph summary should also be shown beside each preview table sample.

1.D2: The details of the user study need to be fleshed out. Tor example, what does it mean to say “important” in “The workers were asked which of the 2 entity types in the pair is more important.”; all responses are valid (no outliers) ?, etc.

(4) related work:
=================================
1.D2: - Other papers to look at, esp. in the context of search:
* "Qunits: queried units in database search", Nandi and Jagadish CIDR 2009
* "Query Biased Snippet Generation in XML Search", Chen et al SIGMOD 2009

3.W3: The related work should be expanded. More discussions about similar work that has been done in the past should be added.

1.W1: Pagerank representation of the graph (ObjectRank, Balmin @ VLDB 2004) 

(5) Clarification and explanantion:
=================================
2.D3: The entity type importance correlation (Section 6.1.3) does not appear to necessarily relevant to the author's preview table goals. There has to be a justification for why that comparison is a reasonable one.

2.D4: Why is Precision@K a good measure for preview quality? At least an informal justification is necessary. Further, the charts in Figure 5 are hard to read and understand.

2.D5: There is no explanation for the non-key attribute scoring in Table 4 not described. In order to measure non-key attribute quality, you could fix the key attributes to be the same as in the gold standard and then let the algorithm only pick the non-key attributes. Then, you can the same metric used to measure quality of key attributes can be used to compare the precision of the non-key attributes.

3.W4: Since the concept of the preview tables for entity graphs are introduced for the first time in this paper, the conclusion should include a thorough discussion about the possibilities of future work.

1.W1: Motivation: the overall motivation for this problem needs to be fleshed out more. While this problem itself may be NP-hard, the authors do not articulate how common this problem is, or what the overall impact is.

(6) execution time:
1.W3: more comparisons against reasonable baselines / comparable algorithms; the evaluations (Fig 6 & 7) simply compare the two proposed algorithms, but do not compare this against a reasonable existing alternative.

(7) Comments we won't address:
=================================
2.W1: Selecting sample tuples is very likely an important part of the puzzle and it was disappointing to see the authors use random sampling.
