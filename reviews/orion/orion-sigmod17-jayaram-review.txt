----------------------- REVIEW 1 ---------------------
PAPER: 349
TITLE: Orion: Enabling Suggestions in a Visual Query Builder for Ultra-Heterogeneous Graphs
AUTHORS: Nandish Jayaram, Rohit Bhoopalam, Chengkai Li and Vassilis Athitsos

Overall evaluation: -1
Reviewer's confidence: 2
Originality: 3
Correctness: 2
Completeness: 2
Best paper award: no

----------- Strong Points -----------
1. Useful approach
 2. Thorough Experimental study

----------- Weak Points -----------
1. The supported query language seems too simple (see
below)
 2. The definition of query sessions is also rather simplistic (see  b below)  3. The mathematical properties of the proposed RDPs need some more  through analysis and justification.

----------- Review -----------
The papers presents Orion, a visual interface for the querying of large heterogenous graphs. In particular, the system supports a recommendation mechanism that provides top-k suggestion for edges that may be added to the query graph. The edge ranking is based on a new concept called Random Decision Paths (RDP) that rank edges based on their co-occurrence in the base data. Experimental study demonstrate the usefulness and performance of the system.
 While such a system may certainly be useful, and the approach overall is interesting, some of the decisions seem to simplistic and some are not well justified or analyzed. The writing may also be improved.


Comments

Definition of Query graph: Common graph query languages include constructs for path expressions (e.g. paths of arbitrary length, "don't care" labels for edges, disjunction ("or") or labels, etc.). These are very useful, in particular when querying heterogenous graphs, but are not supported here. This should be justified (or at least discussed).

Section 3.2 repeats a lot from the introduction. This can be shortened.

Definition of Query Session: It is strange that this is just a set of types. Doesn't it matter in which context (as suggestion to an edged for which exact node in the query graph) a given suggestion was rejected/accepted? One may reject a label in one context by accept it for another node...

The mathematical properties of RDPs need a deeper discussion. What mathematical properties have the RDPs compared to RFs? Do you get the same probabilities/ranking? An approximation within some bound?
Something else?

Page 8, bottom of 1st columns: The injection of  negative edges seems strange - it is not clear in what sense it captures users query intentions.


----------------------- REVIEW 2 ---------------------
PAPER: 349
TITLE: Orion: Enabling Suggestions in a Visual Query Builder for Ultra-Heterogeneous Graphs
AUTHORS: Nandish Jayaram, Rohit Bhoopalam, Chengkai Li and Vassilis Athitsos

Overall evaluation: 1
Reviewer's confidence: 3
Originality: 3
Correctness: 3
Completeness: 2
Best paper award: no

----------- Strong Points -----------
Important Real Problem.  Good problem Definition.

Reasonable solution approach.  Likely to succeed.

Constructed system with nice interface

----------- Weak Points -----------
Lots of little ideas.  Nothing deep.  Innovation is primarily in the problem statement and system architecture.

Many options to consider.  Not all evaluated carefully.

"Ultra-"Heteregeneous.  Really??  
Were you the genius who came up with VeryLargeDB, not being satisfied with just DB or LDB?

----------- Review -----------
You have a real problem, and one hat is truly challenging to solve.
You have a solution that works, and this is not easy.
I do not like your paper in terms of traditional measures of scholarship.
But, I am voting for acceptance nevertheless.

Overall, I think my difficulty with your paper is that it is written like a system description.  You do not get to any technical material until well into page 5.  I suppose that your key problem is suggesting top-k edges.  If so, call it out early, and succinctly explain your solution concept, beginning right in the abstract and introduction.  (Right now, you do mention edge ranking in both, but its intellectual centrality is not clear at this stage, nor is the intuition behind your proposed solution).

You have an important problem of generating edge co-occurrence data, which you address in Sec 5.  But you have multiple ideas that you just throw out, and there is no evaluation.

----------- Required changes for the revision ----------- Rewrite paper to address my detailed comments above (paragraphs 2 and 3).
No new technical material is required, but extensive rewriting is needed.


----------------------- REVIEW 3 ---------------------
PAPER: 349
TITLE: Orion: Enabling Suggestions in a Visual Query Builder for Ultra-Heterogeneous Graphs
AUTHORS: Nandish Jayaram, Rohit Bhoopalam, Chengkai Li and Vassilis Athitsos

Overall evaluation: 0
Reviewer's confidence: 2
Originality: 4
Correctness: 3
Completeness: 2
Best paper award: no

----------- Strong Points -----------
S1. Usability of graph query languages is an important issue, which this paper tackles.
S2. A user study is provided, and is on a non-trivial scale.  
S3. The paper is well-written and easy to follow.

----------- Weak Points -----------
W1. Why are these particular data model and query model chosen as the focus?  Is there a graph model and query language (or a fragment) to which these correspond?  
W2. According to the results of the user study, Orion outperforms a baseline, but statistical significance lacks.  A larger-scale study is necessary to make results more convincing.  Were all students CS PhD students?  How would a general audience perform?
W3. There is quite some distance between outperforming a baseline and being practical.  40 suggestions to compose a relatively small target query is still a high number.  I am not familiar with the HCI literature and the accepted standards there, but how does this result compare to other GUIs of this kind?

----------- Review -----------
See above for a description.  Over-all, this is a non-standard SIGMOD paper - it focuses on HCI issues, and is, indeed, very interesting, and fresh.  What's missing for me is the HCI context - would the approach proposed here be considered practical by the HCI community, taken more broadly than just the part of the community that builds visual graph query systems?


----------------------- REVIEW 4 ---------------------
PAPER: 349
TITLE: Orion: Enabling Suggestions in a Visual Query Builder for Ultra-Heterogeneous Graphs
AUTHORS: Nandish Jayaram, Rohit Bhoopalam, Chengkai Li and Vassilis Athitsos

Overall evaluation: -2
Reviewer's confidence: 3
Originality: 4
Correctness: 1
Completeness: 1
Best paper award: no

----------- Strong Points -----------
Providing recommendations to aid with graph query formulation is an important problem especially when dealing with large online graphs that have millions of nodes and many edge types. A user not familiar with the graph will find a tool like Orion useful to use.

----------- Weak Points -----------
The paper frames itself as a usability paper that aids with query suggestion yet it fails in the following key areas:

W1) Motivating the design decisions
W2) Providing a convincing user evaluation of the results
W3) Demonstrating the effectiveness of any edge suggestion algorithm

I will detail each of these weaknesses below. I feel that if the authors address these issues, they will have a strong and impactful paper. All these issues, however, require a lot of work, which cannot be completed within a revision cycle.

----------- Review -----------
W1) Motivating the design decisions

a) Cluttering the query canvas with suggestions is confusing especially if the user did not initiate any such behavior from the system.
b) Using refresh suggestions to bring in other suggestions is cumbersome especially if the suggestion algorithm is completely off from the user's query intent. 
c) The starting point of querying is an empty canvas and users have to go through a tedious 3-level selection process to add a node of interest.
d) Focusing only on edge suggestions instead of both node and edge suggestions makes the paper feel lacking especially since the introduction makes the reader think that Orion addresses both. Only later do we realize that it doesn't.
e) The interface is very dense with users having to read many suggestions of the form "Click on ...".

Eric Horvitz provides an excellent paper on how to design mixed-initiative interfaces and I strongly encourage the authors to read this paper to use a more principled approach to designing such an interface and to better articulate the choices behind their interface design.

W2) Providing a convincing user evaluation of the results

a) With 30 users, the authors could have produced a stronger sample if they tested both interfaces with users.
b) z-test are typically not used and ANOVA should have been used. 
c) The authors do not account for the same user providing a set of 7 responses. Thus stating that the sample size is 105 responses is misleading as it takes no account of repeated measures.
d) An alpha of 0.1 is too low for such experiments and hence the results appear insignificant.
e) I would have preferred to see how often the users got the exact query graph rather than the graph similarity measure which is lax.
f) What are the significance values for hard and medium alone? Why did the authors combine these values?
g) What makes a query easy vs. hard?
e) What are the exact queries used? Is there a learning effect? How do the authors address this?
h) How much time is spent deciding on a node to add vs. adding an edge?
i) Did the task specifications provide hints to help users. What were these hints?
j) Did the users spend time experimenting with the tool during training time. What tasks did they perform?
k) A visualization of correctness per user would help readers see differences.
l) The likert scale survey is problematic especially since each users responds to seven such surveys! The goal of such surveys is often to get open feedback about the system and there is no room for elaboration.
m) "We observe that Orion users report an improvement of 0.5 for Q1, 0.2 for Q2, 0.25 for Q3 and 0.3 for Q4 on Likert scale, when compared to the Naive users." What were the actual mean values (and distribution)?
n) Asking users about overall satisfaction is also problematic given that users have no familiarity with such tools and cannot gauge if Orion is good or bad? The overall positive responses for both Naive and Orion hint that there might demand characteristics.
o) A steep learning curve especially for a mixed-initiative interface is a negative result and users spending more time on Orion is counterintuitive and points at deep interface design problems.

I strongly recommend the authors to read Jake Wobbrock's "Practical Statistics for HCI".

W3) Demonstrating the effectiveness of any edge suggestion algorithm

This is perhaps one of the strongest problems with the paper. All suggestions appear to require on the order of 60+ suggestions to make a graph. It is unlikely that users will go through that many suggestions to formulate a query. 

Moreover, there are simpler methods of suggestion that could have been used as a baseline to Orion instead of "no suggestions at all". Word similarity auto-suggest for Naive rather than an alphabetical drop-down feels more appropriate as a baseline to compare RDP against.


-------------------------  METAREVIEW  ------------------------
PAPER: 349
TITLE: Orion: Enabling Suggestions in a Visual Query Builder for Ultra-Heterogeneous Graphs

The reviewers decided to reject this paper, but they found both the problem domain and approach interesting.
That said, they found the following issues:

a) The user study section was conducted while ignoring several HCI "best practices", with the methodology being somewhat suspect.
b) As written it's not easy to tease apart the technical challenges and solution insights
c) The solution itself may be somewhat simplistic, without the challenging aspects adequately explored. 
d) Some of the design choices are not adequately justified.
